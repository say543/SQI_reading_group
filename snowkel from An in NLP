07112019

google has its prduction code (not in pytohn)   , snorkel is purely in python.

3 steps
Labeling<snorkel>
    labeling function
       example : two sentences, invite + trigram as signal
        
        
       many functions will apply so output a list of signals
       sort of combine the result
       
       proof: more signals,   more likely your result will be correct.
       
       
       
       only classfier, no regression (real value)
       因為 classfier 可以多數統計dominate 但是regression 不行
       so classfier 會有正確的confidence value (based on statisitical)  但是regression 不行
         ? 有人提到boost 也有應用  or label function 
         ? 有人提到prior
         
       how to deal with high correcltion/ high conflict among label functions
       
       
    snorkel pipelines
    
    problem: noisy , conficting , correclacted
        using generative function 
        0 -> abstain if no label function being applied
        in practice,: suggest function to 50% of data.
        
        
        matrix completetion problem
        ? 要查
          covarairnce matrix
             number of covariance x number of covariance
             
             here claim 沒有depend on number of samples
             ? 有discussion
             
             因為abstain
             
     tanda pipeline
         transformation function
         text
            這樣的transformation 沒有包含semantic, 所以比image 難做
         image
        
         GAN-like network
            TF : transformation questions
            generator to pick 'sample and order'  from transformation functions(functions wrote by users)
            
            seqeunces of TF does matter
            so 可以用LSTM 計history data
            
            
            ? 
            
            feedback from D
               depending on user's object function
               
         discussion
           label first then augmentation
           not
         
         
           descriptor
     slicing data
         ? different from boosting (decision tree)
         i think it should be the same
         easy to train, becasue boost need lots of data to prevent some data representaon from being change
         但是這邊是human ,但是要小心bias
         
         how to prevent overfitting
         ?  like dropout or somethinmg. i guess
         
      using GLUE / super GLUE to benchmarl
      
      
      
augmentaiton<TANDA>
slicing<Metal>
----pay attention to a portion of data,  usin mult-task engine
要先有label 才能agumentation

但是實際的workshop benchmakr 卻沒有用snorkel label 而是用實際的別的labled 好的benchmark
so  performance is unknown





