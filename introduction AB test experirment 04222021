========================================
online material
=========================================

email_title
<Followup: Introduction to Online A/B Testing Class>

Hello,

Thank you for attending yesterday’s Introduction to Online A/B Testing class, presented by the ExP team. We hope you found the content useful. Below are some links to other resources we think you might find useful as you continue your experimentation journey at Microsoft.

If you haven't yet filled out the survey, we would greatly appreciate your feedback at https://aka.ms/introtoexpsurvey

Useful Resources

Yesterday’s slides are available here

We have recently added another class, Designing Experimentation Metrics,” which focuses on the metric design principles we briefly discussed in yesterday’s “Goals, Signals, Metrics” breakout sessions. You can learn more and register at https://aka.ms/exp/metricsclass 

We have an external-facing blog at https://aka.ms/exp/blog where our team publishes about experimentation best practices and interesting learnings from experimentation at Microsoft.

Interested in onboarding your product to the ExP experimentation platform? Email expOnboard@microsoft.com

Documentation about the ExP Team and the ExP Platform lives in the ExP wiki at https://aka.ms/exp/wiki

Did you enjoy the “You’re the Hippo” game? There’s an extended version where you can test your predictive skills against ten previous Microsoft experiments. https://aka.ms/exp/hippo 

We host a biyearly series of talks where we highlight some of the best A/B tests from across Microsoft. To see recordings of past sessions, go to: https://aka.ms/BestExperimentsTalks 

We host a weekly "Experiment Review" meeting for partner teams who request help analyzing experiment results. You can learn more about past experiments and sign up for future reviews at https://aka.ms/expreview

For general questions and discussions about A/B testing, you may want to join the "abdisc" alias in https://idweb 

Again, thank you for joining us yesterday,
Carl and the ExP Team






==================
node 
==================


[hr1]

bucket


ensure casulity 



at the end
A/A
An A/A test is a useful end-to-end test of your entire experimentation system, and a good "sanity check" of your metrics. You can isolate issues with your platform from issues with a specific experiment. Teams at Microsoft typically run a lot of these A/A tests to validate that their setup is working properly.


HEART


[hr2]
slide
https://microsoft.sharepoint.com/:p:/r/teams/expshared/_layouts/15/Doc.aspx?sourcedoc=%7B56708735-1C64-425C-BA60-1E6ABD336BD7%7D&file=2%20-%20Aleksander.pptx&action=edit&mobileredirect=true

video


Carl's A/B test

how to judge alternative hypothesis / null hypothese? find evidence against hypothese  by p-value

If you're curious, Rule of Thumb #7 from this paper by our team quantifies how big "large enough" is based on the distribution of your metric: Seven Rules of Thumb for Web Site Experimenters (exp-platform.com)


how to coumpter p- value
? rarity by many  many time random assignemnt /shuffle and calculate delta
(Because we assumed the null hypothesis is true, we're allowed to shuffle the users.)

 in these simulations, we're not actually re-running the experiment. We're taking the logs from the experiment period and re-shuffling the users' logs offline.



a picutre from centrul theorem




how about you get p =0.047 from A/A ? ()
this is ridiculator

type1 -error: false positives
(coomon  p-value is 0.05)


one question ? 200 / 1000
4% (this is acutally very close to p-value)

click 'insights' bouton to check if you doing A/A or A/B



when should i trust a stat sig movement?

replication experiement
check different audience/data_range


p-value = 0 ,  (might be ?.... very small)
I think it's below 1e-99 where we stop showing p-value. There's about 1e80 particles in the universe, so this p-value should be far beyond "impossible"


why not adjusting p- value, you might miss some real treatment effects(so called power)

power: if the null hypothesis is false, how likely are we to detect it?
'finding power in exp scorescords' you can 
depending on number of traffic/ users

usually 80% column is enough , not look at 95% column


large treatment effect then you observe bigger power.


bigger samples also more accurately estimate the means of the populations.


'Factors Which affect Power'


the lower your metrics variance, the more power you have





type II eerrors : false negative
(this is related to previous power defintion)




prevent sequential comarison


heavy user / all users it might affect you A/B test

different market will have different behavior so only run experiement for the audience you are targeting.



[hr3]
https://microsoft.sharepoint.com/:p:/r/teams/expshared/_layouts/15/Doc.aspx?sourcedoc=%7BBB4089C3-5395-4ACD-A389-DF50F20EB052%7D&file=3%20-%20Online%20-%20Vivek.pptx&action=edit&mobileredirect=true
? still not sure what conuterfactlogging mean

//run concurrentexperiement
'Numberlines' to ioslate experiement from one another.


 
Exp's interaction detection: detect ongoing features


//triggering 
needs counterfactual logging.

counterfactual logging
Counterfactual Logging" means exactly "finding the equivalent for the control group" - we need to log in Control whether users were eligible to see the promo code link
i.e. they didn't see it (because they were in Control) but they were eligible to see it (because they were on the Checkout page with an eligible item in their cart)


segmentation
segments pf onterested tool from EXP

sample ratio mismatched


experimenta tool management
not listening



carryover effect
not listening 


telemetry logging 
not listening 


