
https://arxiv.org/abs/2002.10957

https://microsoft-my.sharepoint-df.com/:v:/p/shagup/ERINj5nU3CpAt0s9XqKTvXEBv48WOeJsBwvfXgnFnR3Kjw

slide deck
http://gabrielilharco.com/publications/EMNLP_2020_Tutorial__High_Performance_NLP.pdf


Hi all,

We’ll have our final presentation of the year this Friday, with Rahul presenting a compressed version of the High Performance Natural Language Processing Tutorial from the recently concluded EMNLP’20. We’ll refresh the series in the new year (around mid. Jan).

1.	High Performance Natural Language Processing [Tutorial Video] [Slides]


[background]
1:49
here to list something more related



also offical link

knowledge distillation
quantization
pruning


efficient-attention


4:30
attention idea

6:00 QKV
based on simialrity to output query

8:35
attention 
scal probelm

  mutli-attenito
  

8:48
positonal encoding 


9:35
3 steops


10:40
[core tech]
'knowledge distillation'
Hinton etak,. 2015 distilling the knowledge in a neural network
basic idea for knowledge distillation
(這邊的teacher model 還沒有pre-trainnig 的概念)

'knowledge distillation for pre-training'
Sanh et ak., 2019
DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
(hugging face)
limitiation:
teacher 是12 layers bert, student 取其中的six layer
student needs to have same arachitecture as teacher (eg: using the 6 layers distil then you use the same architecture)

p.s minLM: a new way to pretrain, check in the future
a new technology, we can take a look  better in architecutre reusing (?)
https://arxiv.org/abs/2002.10957


Sun et al,. 2019
MobileBert: a Compact Task-agnostic BERT for resource-limited Devices
KL convergence
feature map transfer (做一個feature 的projection)
attention transfer 
? both not   can study 

layers 要全用  但是可以thiner 看圖


15:48
'knowledge distillation for fine-tuing'
因為student model 沒辦法用到teacher 的architecutre 因為太大了
所以 不是用pre train 而是fine-tuing fine-tuning

Turc et al., 2019
well-read students learn bette: on the improta

1. regular pretraining
   pick a biggest student you can
   (沒有techer involve)
2. fine-tune student via distillation
   這時候有better teacher 
   ? 這個teacher 的來源沒有說 只有事better teacher 
   (或許我的file 可以這樣用來tune student model  但是這樣是增加unlabel data 而已我猜)
   用到 unlabel data 
3. (optin) regular fine tuning
    label data


'knowledge distillation for pre-training and Fine-tuning'
Jiao et al,. 2019 
TinyBERT: Distilling BERT for naturual langugage undetstanding
1.  pre-training vis distillation
   多做per-layer transfer
   embedding transfer
   ? 沒有說這邊的data 是label or unlabel
   ? 沒有說怎麼做 要看paper 看一下
2. fine-tune student via distillation  (same as 'fine-tune student via distillation' in 'knowledge distillation for fine-tuing')


question:
fine-tune 是不是一定要teacher?
不一定  但是teacher 可以 carry more information
用label data 因該也可以



21:00
quantization

here talks about linear quantization

motivation:
    post-quantization
    train moded then post quantization
    這樣還是會失去精準度
    
so 
Quantization-Aware Training
Jacob et al,. 2017
Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-only inference

forward pass : ^W 對quantized model 計算weight
baackword pass : 對w 計算weight 
? 還不太確定這怎麼計算  可能之後看



zafrir et ak, 2019
Q8ERT: Quantized 8Bit BERT
  針對bert 考慮qunatization 跟accuracy, no detail
  但是不會lost 太多accuracy

Q-BERT: Hessian Based Ultra Kow Preicision Quantization of BERT
  group precision
  比較重要的地方  可以給予比較長的precision
