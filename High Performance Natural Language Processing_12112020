
https://arxiv.org/abs/2002.10957

https://microsoft-my.sharepoint-df.com/:v:/p/shagup/ERINj5nU3CpAt0s9XqKTvXEBv48WOeJsBwvfXgnFnR3Kjw

slide deck
http://gabrielilharco.com/publications/EMNLP_2020_Tutorial__High_Performance_NLP.pdf


Hi all,

We’ll have our final presentation of the year this Friday, with Rahul presenting a compressed version of the High Performance Natural Language Processing Tutorial from the recently concluded EMNLP’20. We’ll refresh the series in the new year (around mid. Jan).

1.	High Performance Natural Language Processing [Tutorial Video] [Slides]


[background]
1:49
here to list something more related



also offical link

knowledge distillation
quantization
pruning


efficient-attention


4:30
attention idea

6:00 QKV
based on simialrity to output query

8:35
attention 
scal probelm

  mutli-attenito
  

8:48
positonal encoding 


9:35
3 steops


10:40
[core tech]
'knowledge distillation'
Hinton etak,. 2015 distilling the knowledge in a neural network
basic idea for knowledge distillation
用teacher model 來tune student model
(這邊的teacher model 還沒有pre-trainnig 的概念)

'knowledge distillation for pre-training'
Sanh et ak., 2019
DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
(hugging face)
limitiation:
teacher 是12 layers bert, student 取其中的six layer
student needs to have same arachitecture as teacher (eg: using the 6 layers distil then you use the same architecture)

p.s minLM: a new way to pretrain, check in the future
a new technology, we can take a look  better in architecutre reusing (?)
https://arxiv.org/abs/2002.10957


Sun et al,. 2019
MobileBert: a Compact Task-agnostic BERT for resource-limited Devices
KL convergence
feature map transfer (做一個feature 的projection)
attention transfer 
? both not   can study 

layers 要全用  但是可以thiner 看圖


15:48
'knowledge distillation for fine-tuing'
因為student model 沒辦法用到teacher 的architecutre 因為太大了
所以 不是用pre train 而是fine-tuing fine-tuning

Turc et al., 2019
well-read students learn bette: on the improta

1. regular pretraining
   pick a biggest student you can
   (沒有techer involve)
2. fine-tune student via distillation
   這時候有better teacher 
   ? 這個teacher 的來源沒有說 只有事better teacher 
   (或許我的file 可以這樣用來tune student model  但是這樣是增加unlabel data 而已我猜)
   用到 unlabel data 
3. (optin) regular fine tuning
    label data


'knowledge distillation for pre-training and Fine-tuning'
Jiao et al,. 2019 
TinyBERT: Distilling BERT for naturual langugage undetstanding
1.  pre-training vis distillation
   多做per-layer transfer
   embedding transfer
   ? 沒有說這邊的data 是label or unlabel
   ? 沒有說怎麼做 要看paper 看一下
2. fine-tune student via distillation  (same as 'fine-tune student via distillation' in 'knowledge distillation for fine-tuing')


question:
fine-tune 是不是一定要teacher?
不一定  但是teacher 可以 carry more information
用label data 因該也可以



21:00
[quantization]

here talks about linear quantization

motivation:
    post-quantization
    train moded then post quantization
    這樣還是會失去精準度
    
so 
Quantization-Aware Training
Jacob et al,. 2017
Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-only inference

forward pass : 對quantized model ^w 計算weight
baackword pass : 對w 計算weight 
? 還不太確定這怎麼計算  可能之後看



zafrir et ak, 2019
Q8ERT: Quantized 8Bit BERT
  針對bert 考慮qunatization 跟accuracy, no detail
  但是不會lost 太多accuracy

Q-BERT: Hessian Based Ultra Kow Preicision Quantization of BERT
  group precision
  比較重要的地方  可以給予比較長的precision
  ? 怎麼選擇沒有說


above tho method
? 可能是 parameter 跟 matrix intermediate result 都是integer 還得看paper 來確定

也是有用到forward pass / backword pass , 所以train 的時候memory 會用的更多, 但是online inference 的時候不會

27:00
quantization with distillation
Zhang et al,. 2020
TernaryBERT: Distillation-ware Ultra-low bit BERT
15 times smaler than original bert
figure right to left

full-precision teacher
full precision student
quantized student

quantized student 計算 forward propagaton + distillation lost (from full-precision teacher) 然後backward propagation to full-precison student
?最後因該要用的是quantized student

30:00
smart reply 有用到quantization
mix precision, also 50% training data reduction
p.s quantization 相較於mix precision, 能夠更好的adapt 而不是只是truncate precision

[31:00]
prune
remove unimportant weights from a network

Pruning  Eayly Work
LeCnu etal., 1990

Hassibi and Stork, 1993
second order derivatives for network pruning: Optimal Brain Surgeon

Pruning based on second-order derivatives
    main idea: start with a reasonably large network
    train it to convergence
    prune in multiple iterations based on second-order derivatives
    OBD: prune and train
    OBS: prune and update weights based on second-order statistics


question : can we TRAIN smaller architecutre instead of pruneing a reasoanly large network?
實現by 下面的paper
Pruning the LTH:
    can we TRAIN smaller architecutre instead of pruneing a reasoanly large network?
    
     Frankile and Carbin, 2019
The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.
