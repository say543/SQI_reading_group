1)	DRN: A Deep Reinforcement Learning Framework for News Recommendation [PDF] – WWW’18
https://nam06.safelinks.protection.outlook.com/?url=http:%2F%2Fwww.personal.psu.edu%2F~gjz5038%2Fpaper%2Fwww2018_reinforceRec%2Fwww2018_reinforceRec.pdf&data=04%7C01%7CChiehChun.Chang%40microsoft.com%7C7f8a213638724e0f646308d710983e55%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636996118453295120%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C-1&sdata=pIJvaAxUs3mPbMDpw4uWAJDobLAGdS6NKnQh4M7Xur0%3D&reserved=0

https://web.microsoftstream.com/video/1d790c3f-ac1d-49c3-9ecc-9e60ee942b47


=================
= my reading
=================

算是collaboraitve 的filter

paper 要 improve的
three major issues. 
First, they only try to model current reward(e.g., Click Through Rate). 
Second, very few studies consider to useuser feedback other than click / no click labels (e.g., how frequentuser returns) to help improve recommendation. 
Third, these meth-ods tend to keep recommending similar news to users, which maycause users to get bored. 

<online framework, 不是offline framework>
we propose to use Deep Q-Learning (DQN) [31] framework.This framework can consider current reward and future rewardsimultaneously. 
The state is defined as featurerepresentation for users and action is defined as feature represen-tation for news. 


Each time when a user requests for news, a staterepresentation (i.e., features of users) and a set of action represen-tations (i.e., features of news candidates) are passed to the agent.The agent will select the best action (i.e., recommending a list ofnews to user) and fetch user feedback as reward.

Specifically, thereward is composed of click labels and estimation of user activeness.All these recommendation and feedback log will be stored in thememory of the agent. Every one hour, the agent will use the log inthe memory to update its recommendation algorithm.



[method 開始講detail]
we use a continuous statefeature representation of users and continuous action feature rep-resentation of items as the input to a multi-layer Deep Q-Networkto predict the potential reward 

用到
Dueling Bandit Gradient Descentexploration strategy

[model framework]

offline part 
用到  user-news clikc label  和user activeness 當作features
? 說有4種  但是只看到兩種
refer to section 4.2
很好的feature talk
news features ,  user features , user news features, context features
實際灌到model
分成state features 跟action features

ser featuresandContext featuresare used asstate features, whileUser news featuresandContext featuresare used as action features


onlie part
agent G 會跟user / network interaction and update
by 
PUSH:
L.Lis generated by combiningthe exploitation of current model (will be discussed in Sec-tion 4.3) and exploration of novel items (will be discussed inSection 4.5).
FEEDBACK:
FEEDBACK:Useruwho has received recommended newsLwill give their feedbackBby his clicks on this set of news.
MINOR UPDATE:
Minor update canhappen after every recommendation impression happens.
After each timestamp (e.g., after times-tampt1), with the feature representation of the previous useruand news listL, and the feedbackB, agentGwill updatethe model by comparing the recommendation performanceof exploitation networkQand exploration network ̃Q(willbe discussed in Section 4.5). If ̃Qgives better recommen-dation result, the current network will be updated towards ̃Q. 
? 要搞清楚Q 跟Q_bar 的差異


MAJOR UPDATE:
Major update usually happens after acertain time interval, like one hour, during which thousandsof recommendation impressions are conducted and theirfeedbacks are collected



[Deep Reinforcenent recommendation]
 we applya Deep Q-Network to model the probability that one user may click on one specific piece of news. 
 
the probability for a user to click on a pieceof news (and future recommended news) is essentially the rewardthat our agent can get. Therefore, we can model the total reward asEquation 1


We use survival models [18,30] to model user return and useractiveness. Survival analysis [18,30] has been applied in the fieldof estimating user return time [20]. 

=================
= discussion
=================
有reinforcement learning 的basic introduction
