
======================
talk / questions
======================
Introduction? I was hoping for a deep dive. 😃

Profile picture of Mei-Yuh Hwang.There is too much details that Xinjie is cu... by Mei-Yuh Hwang

Mei-Yuh Hwang
3:34 PM

👍
2
There is too much details that Xinjie is cutting out. So he is calling it "introduction"
Profile picture of Milad Shokouhi.brb by Milad Shokouhi

Milad Shokouhi
3:34 PM

👍
1
brb

back but have a hard stop at 4pm unfortunat... by Milad Shokouhi

Milad Shokouhi3:38 PM
back but have a hard stop at 4pm unfortunately. I'll watch the 2nd half of video offline

Profile picture of Mei-Yuh Hwang.Where is the pseudo parallel text from? by Mei-Yuh Hwang

Mei-Yuh Hwang
3:40 PM
Where is the pseudo parallel text from?

(en-us, MT(en-us)) ? by Mei-Yuh Hwang

Mei-Yuh Hwang3:40 PM
👍
1
(en-us, MT(en-us)) ?

FYI: Giza++ is an open source for training ... by Mei-Yuh Hwang

Mei-Yuh Hwang3:41 PM
FYI: Giza++ is an open source for training MT and run-time inference, which is able to output the translation alignment, usually at phrase leve.

"report" can be translated into multiple wa... by Mei-Yuh Hwang

Mei-Yuh Hwang3:43 PM
"report" can be translated into multiple ways in Chinese. Without domain knowledge, MT usually does not do well.

Profile picture of Mikkel Conradi.Does this Machine Translation approach acco... by Mikkel Conradi

Mikkel Conradi
3:44 PM
Edited
Does this Machine Translation approach account for the fact that some languages have a very different structure than English? For example, in German, the verb is often the last word in the sentence.

Profile picture of Mei-Yuh Hwang.Yes, it's all data-driven. by Mei-Yuh Hwang

Mei-Yuh Hwang
3:44 PM

👍
1
Yes, it's all data-driven.

Not by syntatic rules by Mei-Yuh Hwang

Mei-Yuh Hwang3:44 PM
Not by syntatic rules

Profile picture of Shuyin Zhao.is that what the translation alignment tool... by Shuyin Zhao

Shuyin Zhao
3:45 PM
is that what the translation alignment tool like Giza++ does?

Profile picture of Mei-Yuh Hwang.Name entities should be locale-appropriate,... by Mei-Yuh Hwang

Mei-Yuh Hwang
3:45 PM
Name entities should be locale-appropriate, rather than direct translation.

Giza++ contains both a trainer to train an ... by Mei-Yuh Hwang

Mei-Yuh Hwang3:46 PM
👍
1
Giza++ contains both a trainer to train an MT model. Also run-time inference, whose output contains (1) translation output, (2) phrase alignment, such as (good morning, 早上好)

Actually Giza++ contains multiple trainers/inference engines using different algorithms

for satori entity, i suppose you query satori graph it will give you a list of candidates. (for example : alice) . is any processing or filtering being applied  to select from a returened list ? thanks.

Chieh-Chun Chang
for satori entity, i suppose you query satori graph it will give you a list of candidates. (for example : alice) . is any processing or filtering being applied  to select from a returened list ? thanks.
Query by entity type; usually you can apply all the returned entity strings in your data augmentation.
    
for multi-state distillation: does that mean each state you only back propgation with respoect to a specific lost?  how those losts being decided in order?  thanks!
​

What translated names are used is not as important, once you start to use PCG as one of your input features, which Xinjie did some experiments, but not sure if he has time to talk about.

The idea is that this particular deep learning model will at some point substitute all the current LU models in production? In order to make I18n (and maintenance) simpler for all models? Or is this specific to some set of models/products? Or we don't know yet?  It is not clear to me how the deep learning model shown fits with the other existing or upcoming models.

Sorry have to drop; thanks Xinjie. Please note that TULRv3 should be available for experimentation too and is going to be very similar to v2. Also we have implementations for both wordpiece and sentencepience. I think you're already connected, but please let me know if I can put you in touch for more details

as more langs & domains added to same universal model, distilled student model may need to get larger

Yes, that's one way to do it. But need to make sure it doesn't have regression on existing domains.

The direction is to ship universal models for all UU scenarios, at least for all the non-En locales.


    
For en-us, Yue is working on TNLR for PME. So I can imagine more and more our models will become transformer-based.
​
    
However, if it's a search scenario, we have to ship it on B2 FPGA, which has limited capacity. 
​
    
Each domain has its own adapter, but shares the pre-trained model for representation.



    
Since ORCA is using adapter, they are also asking Brainwave team to consider FPGA support.
​


    
reduction by 130MB, or reduced TO 130MB?
​[4:14 PM] Mei-Yuh Hwang
    
BTW, these are on top of the student model (after distilled from the big model), correct?
​[4:15 PM] Yang Yang (MSAI)
    
correct
​[4:15 PM] Mei-Yuh Hwang
    
FPGA quantization: 16bit or 8bit?
​[4:16 PM] Kieran McDonald
    
I think they support both and msfp but could be wrong
​[4:20 PM] Mei-Yuh Hwang
    
still in SDFv2 limited availability to a small set of users
​[4:23 PM] Chieh-Chun Chang
    
for multi-state distillation: does that mean each state you only back propgation with respoect to a specific lost?  how those losts being decided in order?  thanks!


[4:27 PM] Mei-Yuh Hwang
    
+1 using unlabeled data!!!
(2 liked)Edited​[4:31 PM] Mei-Yuh Hwang
    
Thank you, Xinjie. I have to drop for my next meeting.
​[4:31 PM] Yang Yang (MSAI)
    
have to drop. thank you XInjie
​[4:31 PM] Mei-Yuh Hwang
    
+1 using unlabeled data
​[4:34 PM] Donald Huo
    Have to drop now. Thanks, Xinjie
​[4:34 PM] Kieran McDonald
    
Great work and presentation!
​[4:34 PM] Mikkel Conradi
    
Thanks Xinjie!
​[4:35 PM] Kieran McDonald
    
thank you
​[4:35 PM] Yue Ma
    Thanks Xinjie!

​[4:43 PM] Xinjie Zhou
    
Chieh-Chun Chang
12,532  paparmeters accounts for just sinlge domain or multiple domains? thanks.
7 domains, each domain has a different adapter. We used TVS dataset in that experiment.
Edited


    
Chieh-Chun Chang
12,532  paparmeters accounts for just sinlge domain or multiple domains? thanks.
It's the number for just one adaptor structure.



+1 using unlabeled data!!!
​
    
Thank you, Xinjie. I have to drop for my next meeting.
​
    
have to drop. thank you XInjie
​
    
+1 using unlabeled data
​
    Have to drop now. Thanks, Xinjie
​
    
Great work and presentation!
​
    
Thanks Xinjie!
​
    
thank you
​
    Thanks Xinjie!





summary question
[1]
for multi-state distillation: does that mean each state you only back propgation with respoect to a specific lost?  how those losts being decided in order?  thanks!
Yes, multi-stage distillation means we use different loss functions in different training stages. 
We conducted many experiments to compare one loss vs. multiple loss and the training order of different loss.


[2]
Chieh-Chun Chang
12,532  paparmeters accounts for just sinlge domain or multiple domains? thanks. 這個就是adapter
adapter 的相關東西
7 domains, each domain has a different adapter. We used TVS dataset in that experiment

[3]
Since ORCA is using adapter, they are also asking Brainwave team to consider FPGA support.

[4]
Sorry have to drop; thanks Xinjie. 
Please note that TULRv3 should be available for experimentation too and is going to be very similar to v2. Also we have implementations for both wordpiece and sentencepience. I think you're already connected, but please let me know if I can put you in touch for more details


[5]
[Yesterday 3:49 PM] Yang Yang (MSAI)
    
Chieh-Chun Chang
for satori entity, i suppose you query satori graph it will give you a list of candidates. (for example : alice) . is any processing or filtering being applied  to select from a returened list ? thanks.
Query by entity type; usually you can apply all the returned entity strings in your data augmentation.
​[Yesterday 3:50 PM] Mei-Yuh Hwang
    
What translated names are used is not as important, once you start to use PCG as one of your input features, which Xinjie did some experiments, but not sure if he has time to talk about.
Edited


======================
======================

1

TNLR v1 24 layers
500k
teach model


15:00 model archutecture
CLS + token embedding
domain softmax on CLS , 就是input 的token embeding 
intent : softmax on CLS

slot : token embeeding on top of bi-LSTM CRF
? 在想我的作法因該也是bert 使用的第二種做法 fine turn bert with extra layer 而不是第一種作法 用embedding 的reult 直接給utput layer


27:00
q1: universak model v.s langugage-specific model ? which one is better
conclusion : universal model can perform better than langugae-specifc model

q2: could we add new domains without impacting existing ones?
simple way : fix parameters, only fine-tune the doamin specific parameters(people)
but performance is not good, so we use adapter

29:56
adapter (a new model introduced in transformer's block)
31:26
fix transformer paramters, but fine tune adapter parameter models
'papameter efficient adapter'
but might not be very efficient snice 10 domains you need to add whole, more not be very efficent

thus, go with this 
'light-wegiht adapter from ORCA'
light- weight adapter is outside of transformer
it aggregates the output the all transformer layers' output

adapter 
weghted sum + stacked convolution + full connect  => then output to eac domain

at the same time, pretrained model will be fixed. will not need to fine tune 
? but not sure how many layers


35:25
Mbert
<comaprison>
adapter use all 12 layers output + enmbedding output
? what does that mean
fine tune bottom 6 layers and train the top 6 layers


37:04
more on adapter
share hardware ?
but adpter structre is not supported on FPGA right now.



40:32
vocab pruning




42:30
Konwledge distillation
teacher(1024) / student (768) dimention


weight initialzation
    student with teacher initialization provide better performance
    truncation method - use sub dimention 
knowledge distillation
     hard loss : golde labe from data annotation
     soft loss : learn prediceted distribution from teacher
     intermeridate loss : intermediate loss
       all is applied in multi-state traninig , in a order
     order : intermediate -> hard -> soft (by many experiments then have this model)
     
 additionla data
    levarge unlabelde data for distillation.
     

how Teacher 


lexicon feature
?  not in M1

if including, need manually conversaion. (but not covered in this presentation)


wordpiecetomenizer5.md
? 感覺是merge slot 在不同language 才有影響



